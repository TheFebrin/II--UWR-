{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from collections import OrderedDict \n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from typing import *\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_corpora_path = '../../../polish_corpora.txt'\n",
    "poleval2_path = '../../../poleval_2grams.txt'\n",
    "poleval3_path = '../../../poleval_3grams.txt'\n",
    "supertags_path = 'supertags.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text).rstrip().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 23011601/23011601 [06:39<00:00, 57549.97it/s]\n"
     ]
    }
   ],
   "source": [
    "unigrams: Mapping[bytes, int] = {}  # word -> number of occurrences\n",
    "\n",
    "with open(polish_corpora_path, encoding=\"utf8\") as f:\n",
    "    for line in tqdm(f, desc='Loading data...', position=0, leave=True, total=23011601):\n",
    "        for word in regex(text=line):\n",
    "            _word = bytes(bytearray(word, 'UTF-8'))\n",
    "            if _word in unigrams:\n",
    "                unigrams[_word] += 1\n",
    "            else:\n",
    "                unigrams[_word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_polish_letters(word: str) -> str:\n",
    "    assert type(word) == str, 'Wrong type!'\n",
    "    polish = 'ąćęłńóśźż'\n",
    "    tokenized = 'acelnoszz'\n",
    "    translator = str.maketrans(polish, tokenized)\n",
    "    return word.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 23011601/23011601 [17:48<00:00, 21537.92it/s]\n"
     ]
    }
   ],
   "source": [
    "word_to_tokenize: Mapping[bytes, Set[bytes]] = {}\n",
    "\n",
    "with open(polish_corpora_path, encoding=\"utf8\") as f:\n",
    "    for line in tqdm(f, desc='Loading data...', position=0, leave=True, total=23011601):\n",
    "        for word in regex(text=line):\n",
    "            tokenized = change_polish_letters(word)\n",
    "            _word = bytes(bytearray(word, 'UTF-8'))\n",
    "            _tokenized = bytes(bytearray(tokenized, 'UTF-8'))\n",
    "            if _tokenized in word_to_tokenize:\n",
    "                word_to_tokenize[_tokenized].add(_word)\n",
    "            else:\n",
    "                word_to_tokenize[_tokenized] = set([_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_tokenized_text(text: str) -> str:\n",
    "    \"\"\" Asserts that text is tokenized using:\n",
    "            1. regex\n",
    "            2. change_polish_letters\n",
    "    \"\"\"\n",
    "    reconstructed = []\n",
    "    for word in text.strip().split():\n",
    "        word = bytes(bytearray(word, 'UTF-8'))\n",
    "        if word in word_to_tokenize:\n",
    "            possible_reconstructions: Set = word_to_tokenize[word]\n",
    "            reconstructions_with_scores: List[Tuple[bytes, int]] \\\n",
    "                = [(x, unigrams[x]) for x in possible_reconstructions]\n",
    "            best_word = max(reconstructions_with_scores, key=lambda x: x[1])\n",
    "            reconstructed.append(best_word[0].decode(\"UTF-8\") )\n",
    "        else:\n",
    "            reconstructed.append('?')\n",
    "    reconstructed[0] = reconstructed[0].capitalize()\n",
    "    reconstructed[-1] += '.'\n",
    "    return ' '.join(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Może będę robić coś śmiesznego.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_tokenized_text(text='moze bede robic cos smiesznego')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
